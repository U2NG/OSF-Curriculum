# Introducing the Problem

## Learning objectives
* Understand the current issues and barrier to reproducibility
* Understand why the whole workflow is important
* Understand documentation/organizational issues underpinning this

## Section Outline
* Discuss evidence for low levels of reproducibilty

Over the past few years there has been a growing concern that many of the findings in the published literature may now replicate. 

* What is reproducibility?

There are a lot of different definitions out there of what ‘reproducible’ means. So before we get too far we should define out terms. 

One way in which a study can be reproducible is that if you took someones data and their code and reran exactly what they did, you would be able to reproduce the numbers they got. This is sometimes called computational reproducibility. Another type of reproducibility is the question of whether we can reproduce what was done. So, do we have enough information to rerun the experiment or survey the way it was originally done? Another type of reproducibility is the question of, if we reran the study using the same methods and the same analyses, but collected our own data, would we reproduce the results? This is often called replicability. 

Now, all these types of reproducibility are important and all have their challenges. And you’ll notice that some are dependent upon one another. So, if you don’t have experimental reproducilbity you won’t be able to gage the replicability of a study since you won’t ever be able to rerun it. Today, we’ll mostly be talking about ways to increase the computational and experimental reproducibility of studies. 

* Why is this a problem (for individual researchers, for a lab, for others)

Low levels of reproducibility are a problem for a number of reasons. One of the ones you’ve probably heard before is that it’s a problem because if findings don’t replicate, that means we may not know as much as we think we know, and if we can’t reproduce the experiments, we can’t even check if the results will replicate, so it makes it difficult to check and shore up our knowledge. 

However, low levels of reproducibility also have an impact closer to home, specifically, it can make work in your own lab less efficient. If you think how transient labs are, often one graduate student starts something, leaves, and then another graduate student is trying to pick up their project or build an extend a project done by someone else. Or perhaps you submit a paper, and 6 months later an editor asks you for new analyses and you have to go and find your data and analysis findings again. Both of these scenarios are contingent on knowing where all your stuff is, and a large amount of either transfer of knowledge or remembering of knowledge, which in general we don’t do well right now. This means that it will be hard to replicate your own work, which can lead to unneccsaring time and resources being wasted. 


* What is leading to low levels of reproducibility?
	* Current workflow non-transparency
	* Current lack of workflow documentation
		* issues that can arrive with lack of transparency at all points
	* Show a real world example of ‘typical’ workflow with really bad organization
